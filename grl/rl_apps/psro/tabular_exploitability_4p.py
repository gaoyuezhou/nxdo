import logging
import os
import time
from typing import Dict, Type, List

import deepdish
import numpy as np
import ray
from ray.rllib.agents.callbacks import DefaultCallbacks
from ray.rllib.agents.dqn import DQNTrainer
from ray.rllib.env import BaseEnv
from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker
from ray.rllib.policy import Policy
from ray.rllib.utils import merge_dicts, try_import_torch
from ray.rllib.agents.ppo.ppo import PPOTrainer
from ray.rllib.agents.ppo.ppo_torch_policy import PPOTorchPolicy, \
    KLCoeffMixin as TorchKLCoeffMixin, ppo_surrogate_loss as torch_loss
from ray.rllib.evaluation.postprocessing import compute_advantages, \
    Postprocessing
from ray.rllib.models import ModelCatalog
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.policy.tf_policy import LearningRateSchedule, \
    EntropyCoeffSchedule
from ray.rllib.policy.torch_policy import LearningRateSchedule as TorchLR, \
    EntropyCoeffSchedule as TorchEntropyCoeffSchedule
from ray.rllib.utils.framework import try_import_torch
from ray.rllib.utils.test_utils import check_learning_achieved
from ray.rllib.utils.tf_ops import explained_variance, make_tf_callable
from ray.rllib.utils.torch_ops import convert_to_torch_tensor
from ray.rllib.models import MODEL_DEFAULTS

from ray.rllib.utils import merge_dicts
from tables.exceptions import HDF5ExtError
import argparse
import re
import traceback 
import sys


from grl.rl_apps.scenarios.catalog import scenario_catalog
from grl.rl_apps.scenarios.psro_scenario import PSROScenario
from grl.rl_apps.scenarios.ray_setup import init_ray_for_scenario
from grl.rl_apps.scenarios.stopping_conditions import StoppingCondition
from grl.rllib_tools.space_saving_logger import get_trainer_logger_creator
from grl.envs.poker_4p_multi_agent_env  import Poker4pMultiAgentEnv

from grl.rllib_tools.policy_checkpoints import load_pure_strat
from grl.rl_apps.psro.poker_utils import openspiel_policy_from_nonlstm_rllib_policy
from grl.rl_apps.scenarios.trainer_configs.poker_4p_psro_configs import *
from grl.rl_apps.tiny_bridge_4p_mappo import CCTrainer_4P, CCPPOTorchPolicy_4P

from grl.utils.common import pretty_dict_str, datetime_str, ensure_dir
from grl.utils.strategy_spec import StrategySpec
from grl.rl_apps.scenarios.stopping_conditions import *
from grl.rl_apps.tiny_bridge_4p_mappo import *
from grl.rl_apps.scenarios.trainer_configs.poker_psro_configs import *
from grl.rl_apps.scenarios.trainer_configs.bridge_psro_configs import *
from grl.algos.p2sro.payoff_table import PayoffTable
from grl.algos.p2sro.p2sro_manager.utils import get_latest_metanash_strategies, PolicySpecDistribution
from grl.rllib_tools.models.valid_actions_fcnet import get_valid_action_fcn_class_for_env
from grl.rl_apps.centralized_critic_model import TorchCentralizedCriticModel

import pyspiel

torch, _ = try_import_torch()


logger = logging.getLogger(__name__)


def checkpoint_dir(trainer):
    return os.path.join(trainer.logdir, "br_checkpoints")


def load_metanash_pure_strat(policy: Policy, pure_strat_spec: StrategySpec):
    pure_strat_checkpoint_path = pure_strat_spec.metadata["checkpoint_path"]
    checkpoint_data = deepdish.io.load(path=pure_strat_checkpoint_path)
    weights = checkpoint_data["weights"]
    weights = {k.replace("_dot_", "."): v for k, v in weights.items()}
    policy.set_weights(weights=weights)
    policy.p2sro_policy_spec = pure_strat_spec


def update_all_workers_to_latest_metanash(trainer: DQNTrainer,
                                          metanash_policy_specs,
                                          metanash_weights,
                                          ):
    def _set_opponent_policy_distribution_for_one_worker(worker: RolloutWorker):
        worker.metanash_policy_specs = metanash_policy_specs
        worker.metanash_weights = metanash_weights
    trainer.workers.foreach_worker(_set_opponent_policy_distribution_for_one_worker)


class P2SROPreAndPostEpisodeCallbacks(DefaultCallbacks):

    def on_episode_start(self, *, worker: RolloutWorker, base_env: BaseEnv,
                         policies: Dict[str, Policy],
                         episode: MultiAgentEpisode, env_index: int, **kwargs):
        # Sample new pure strategy policy weights from the metanash of the subgame population for the best response to
        # train against. For better runtime performance, consider loading new weights only every few episodes instead.
        metanash_policy: Policy = policies[f"metanash"]
        metanash_policy_specs = worker.metanash_policy_specs
        metanash_weights = worker.metanash_weights

        new_pure_strat_spec: StrategySpec = np.random.choice(a=metanash_policy_specs, p=metanash_weights)
        # noinspection PyTypeChecker
        load_metanash_pure_strat(policy=metanash_policy, pure_strat_spec=new_pure_strat_spec)


def train_poker_approx_best_response_psro(br_team,
                                          ray_head_address,
                                          scenario_name,
                                          general_trainer_config_overrrides,
                                          br_policy_config_overrides: dict,
                                          get_stopping_condition,
                                          metanash_policy_specs,
                                          metanash_weights,
                                          results_dir,
                                          print_train_results=True):
    scenario: PSROScenario = scenario_catalog.get(scenario_name=scenario_name)

    env_class = scenario.env_class
    env_config = scenario.env_config
    trainer_class = scenario.trainer_class
    policy_classes: Dict[str, Type[Policy]] = scenario.policy_classes
    p2sro = scenario.p2sro
    get_trainer_config = scenario.get_trainer_config
    psro_get_stopping_condition = scenario.psro_get_stopping_condition
    mix_metanash_with_uniform_dist_coeff = scenario.mix_metanash_with_uniform_dist_coeff

    other_team = 1 - br_team

    br_learner_name = f"new_learner_{br_team}"

    def log(message, level=logging.INFO):
        logger.log(level, f"({br_learner_name}): {message}")

    def select_policy(agent_id):
        if agent_id % 2 == br_team:
            return "best_response"
        elif agent_id % 2 == other_team:
            return "metanash"
        else:
            raise ValueError(f"Unknown agent id: {agent_id}")

    init_ray_for_scenario(scenario=scenario, head_address=ray_head_address, logging_level=logging.INFO)

    tmp_env = env_class(env_config=env_config)

    trainer_config = {
        "callbacks": P2SROPreAndPostEpisodeCallbacks,
        "env": env_class,
        "env_config": env_config,
        "gamma": 1.0,
        "num_gpus": 0,
        "num_workers": 0,
        "num_envs_per_worker": 1,
        "multiagent": {
            "policies_to_train": [f"best_response"],
            "policies": {
                f"metanash": (
                policy_classes["metanash"], tmp_env.observation_space, tmp_env.action_space, {"explore": scenario.allow_stochastic_best_responses}),
                f"best_response": (policy_classes["best_response"], tmp_env.observation_space, tmp_env.action_space,
                                   br_policy_config_overrides),
            },
            "policy_mapping_fn": select_policy,
        },
    }

    trainer_config = merge_dicts(trainer_config, get_trainer_config(tmp_env))
    trainer_config = merge_dicts(trainer_config, general_trainer_config_overrrides)
    # trainer_config["rollout_fragment_length"] = trainer_config["rollout_fragment_length"] // max(1, trainer_config["num_workers"] * trainer_config["num_envs_per_worker"] )

    trainer = trainer_class(config=trainer_config,
                            logger_creator=get_trainer_logger_creator(base_dir=results_dir, scenario_name="approx_br", should_log_result_fn=lambda result: result["training_iteration"] % 100 == 0))
    update_all_workers_to_latest_metanash(trainer=trainer,
                                          metanash_policy_specs=metanash_policy_specs,
                                          metanash_weights=metanash_weights)

    train_iter_count = 0

    stopping_condition: StoppingCondition = get_stopping_condition()
    max_reward = None
    while True:
        train_iter_results = trainer.train()  # do a step (or several) in the main RL loop
        train_iter_count += 1

        if print_train_results:
            train_iter_results["best_response_player"] = br_team
            # Delete verbose debugging info before printing
            if "hist_stats" in train_iter_results:
                del train_iter_results["hist_stats"]
            if "td_error" in train_iter_results["info"]["learner"][f"best_response"]:
                del train_iter_results["info"]["learner"][f"best_response"]["td_error"]
            print(pretty_dict_str(train_iter_results))

        total_timesteps_training_br = train_iter_results["timesteps_total"]
        total_episodes_training_br = train_iter_results["episodes_total"]
        br_reward_this_iter = train_iter_results["policy_reward_mean"][f"best_response"]
        if max_reward is None or br_reward_this_iter > max_reward:
            max_reward = br_reward_this_iter
        if stopping_condition.should_stop_this_iter(latest_trainer_result=train_iter_results):
            break

    trainer.cleanup()
    ray.shutdown()
    time.sleep(10)

    return max_reward


def select_policy(agent_id):
    if agent_id % 2 == team:
        return "best_response"
    elif agent_id % 2 == other_team:
        return "metanash"
    else:
        raise ValueError(f"Unknown agent id: {agent_id}")

def get_policy_from_scenario(scenario_name):
    scenario: PSROScenario = scenario_catalog.get(scenario_name=scenario_name)
    if not isinstance(scenario, PSROScenario):
        raise TypeError(f"Only instances of {PSROScenario} can be used here. {scenario.name} is a {type(scenario)}.")
    get_trainer_config = scenario.get_trainer_config

    env_class = scenario.env_class
    env_config = scenario.env_config
    trainer_class = scenario.trainer_class
    policy_classes: Dict[str, Type[Policy]] = scenario.policy_classes
    tmp_env = env_class(env_config=env_config)

    trainer_config = {
        "callbacks": P2SROPreAndPostEpisodeCallbacks,
        "env": env_class,
        "env_config": env_config,
        "gamma": 1.0,
        "num_gpus": 0,
        "num_workers": 0,
        "num_envs_per_worker": 1,
        "multiagent": {
            "policies_to_train": [f"best_response"],
            "policies": {
                f"metanash": (
                policy_classes["metanash"], tmp_env.observation_space, tmp_env.action_space, {"explore": False}),
                f"best_response": (
                policy_classes["best_response"], tmp_env.observation_space, tmp_env.action_space, {}),
            },
            "policy_mapping_fn": select_policy,
        },
    }

    trainer_config = merge_dicts(trainer_config, get_trainer_config(tmp_env))
    trainer = trainer_class(config=trainer_config,
                            logger_creator=get_trainer_logger_creator(
                                base_dir="./junk", scenario_name=scenario_name,
                                should_log_result_fn=True))

    policy = trainer.workers.local_worker().policy_map["metanash"]
    return policy

def get_population_distribution_for_player(player, payoff_table):
    population_meta_strategies: Dict[int, PolicySpecDistribution] = get_latest_metanash_strategies(
        payoff_table=payoff_table,
        as_player=1-player,  # get strategies for opponent to 1, so player 0's metanash mixed strat
        as_policy_num=-1,  # get whole payoff table
        fictitious_play_iters=int(os.getenv("FSP_ITERS", 2000)),
        mix_with_uniform_dist_coeff=0.0
    )

    # this is a mixed strategy below, we sample StrategySpecs from it
    policy_distribution: PolicySpecDistribution = population_meta_strategies[player]  # player zero strategy
    return policy_distribution.probabilities_for_each_strategy()

if __name__ == "__main__":
    ray.init(log_to_driver=True)
    base_path = "/home/mcaleste/Documents/Research/Multiagent/team_psro/psro_checkpoints/"

    # payoff_table = PayoffTable.from_json_file(
    #     json_file_path = base_path + "manager_11.57.27AM_May-11-2022/payoff_table_checkpoints/payoff_table_checkpoint_latest.json")

    # pop_distribution0 = get_population_distribution_for_player(0, payoff_table)
    # pop_distribution1 = get_population_distribution_for_player(1, payoff_table)

    # print(pop_distribution0)
    # print(pop_distribution1)

    scenario_name = "tiny_bridge_4p_psro"
    rllib_policy = get_policy_from_scenario(scenario_name)

    import pdb; pdb.set_trace()

    # # way to get a spec 1
    # strategy_spec_from_payoff_table_for_player_and_index: StrategySpec = opponent_population_payoff_table.get_spec_for_player_and_pure_strat_index(
    #     player=0, pure_strat_index=5)
    #
    # # way to get a spec 2
    list_of_strategy_specs_team_0: List[StrategySpec] = payoff_table.get_ordered_spec_list_for_player(player=0)
    list_of_strategy_specs_team_1: List[StrategySpec] = payoff_table.get_ordered_spec_list_for_player(player=1)

    # way to get a spec 3
    # spec: StrategySpec = opponent_protagonist_policy_distribution.sample_policy_spec()

    spec = list_of_strategy_specs_team_0[0]
    marvel_path = spec.metadata["checkpoint_path"]
    local_path = base_path + marvel_path[60:]
    spec.metadata["checkpoint_path"] = local_path

    load_metanash_pure_strat(rllib_policy, pure_strat_spec=spec)
    print(rllib_policy, "pure rllib strat")

    openspiel_game = pyspiel.load_game("kuhn_poker", {'players': 4})
    single_openspiel_policy = openspiel_policy_from_nonlstm_rllib_policy(openspiel_game=openspiel_game,
                                                                         rllib_policy=rllib_policy,
                                                                         game_version="kuhn_poker",
                                                                         game_parameters={"players": 4},
                                                                         )
    print(single_openspiel_policy)
